% !TEX program = xelatex
% !TEX options = --shell-escape -synctex=1 -interaction=nonstopmode -file-line-error "%DOC%"


\documentclass{nrfh-ish}

%\usepackage[utf8]{inputenc}
%\usepackage[spanish,es-noshorthands]{babel}
\usepackage{color}
\usepackage[x11names, svgnames, rgb]{xcolor}
\usepackage{tikz}
\usetikzlibrary{snakes,arrows,shapes,babel,calc}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{booktabs, multirow} % for borders and merged ranges
\usepackage{soul}% for underlines
\usepackage{changepage,threeparttable}
\usepackage{float}
\usepackage{listings}
\usepackage{pdfpages}
\usepackage{minted}
\usepackage[spanish]{babel}
\usepackage{schemata}
\usepackage{longtable}

\ExplSyntaxOn

\NewDocumentCommand{\getenv}{om}
{
  \sys_get_shell:nnN{ kpsewhich ~ --var-value ~ #2 }{}#1
}

\ExplSyntaxOff
\getenv[\MATRICULA]{MATRICULA}

\usepackage{fontspec}
\setmonofont{Fira Code}[
    Contextuals = Alternate,
    Ligatures = TeX,
]

\lstset{
  basicstyle=\ttfamily,
  extendedchars=true,
  inputencoding=utf8,
  literate=
    {}{{ $\coloncolon$ }}1 {}{{ $\rightarrow$ }}1
    {}{{ $\mathbb{N}$ }}1 {}{{ $\equiv$ }}1
    {}{{ $\forall$ }}1 {位}{{$\lambda$}}1
    {恕}{{ $\textsuperscript{l}$ }}1
    {食}{{ $\textsuperscript{r}$ }}1
    {}{{ $\cdot$ }}1
    {}{{ $\textsubscript{2}$ }}1
    {}{{ $\stackrel{?}{=}$ }}1
    {}{{ $\times$ }}1
    {}{{ $\textsubscript{0}$ }}1
    {}{{ $\textsubscript{1}$ }}1
    ,
  captionpos=b,
  breaklines=true,
  columns=fullflexible,
}

\newcommand\AB[2]{\schema{\schemabox{#1}}{\schemabox{#2}}}

\newcommand\Par[1]{\begin{tabular}{p{9.5cm}}#1\end{tabular}}

  
%\usepackage{listings-rust}

%NIGHT MODE 
%\pagecolor[rgb]{0.1,0.1,0.1}  
%\color[rgb]{1,1,1} 

%\usetikzlibrary{topaths,arrows,automata, positioning,shapes.multipart,shapes}
\newcount\mycount

%\graphicspath{{./}}

%\theoremstyle{definition}
\newtheorem{definition}{Definici\'on}[section]
\newtheorem{proposition}[definition]{Proposici\'on}
\newtheorem{lemma}[definition]{Lema}
\newtheorem{theorem}[definition]{Teorema}
\newtheorem{corollary}[definition]{Corolario}
\newtheorem{example}[definition]{Ejemplo}
\newtheorem{observation}[definition]{Observaci\'on}
\newtheorem{problem}{Problema}
\newtheorem{question}[definition]{Pregunta}
\newtheorem{pointt}{}

\def\proof{\noindent{\textbf{Demostraci\'on}}\\}
\def\endproof{\hfill{\ensuremath\square}}
\def\refname{Referencias}
\allowdisplaybreaks

\title{Proyecto Final: Similitud Coseno}
\titlesub{Recuperaci贸n de Informaci贸n\\
Beatriz Beltr谩n Mart铆nez\\
Licenciatura en Ciencias de la Computaci贸n\\
Primavera 2022}
\headertitle{Proyecto Final: Similitud Coseno}
%\title{Correlacion de }

\author{\authormacro{Gustavo Iv谩n Molina Rebolledo}{\MATRICULA\\Benem茅rita Universidad Aut贸noma de Puebla}
}
\headerauthor{I. Molina-Rebolledo}

\date{\today}


\begin{document}
\maketitledoble

%\begin{abstract帽}
%    Descripci贸n breve de ideas de proyectos.\end{abstract帽}

\begin{abstract}
  In this paper, we present a Haskell program that calculates the similarity
  between every pair of given documents. The similarity is calculated using
  the cosine similarity. The documents are given as text files which are
  parsed and stored in a data structure. Then the program calculates the
  number of ocurrences of each word in the documents and the respective
  weight for each word. All of this is done in Haskell except the stemming
  process which is done through the use of a C library. We also discuss the
  performance of the program and the posible improvements that could be
  made.
\end{abstract}

\makedate

%BEGIN DOCUMENT
\clearpage

\section{Introducci贸n}
Este trabajo retoma las ideas que se han presentado en los trabajos anteriores.
Sin embargo, se est谩 tomando en cuenta una nueva organizaci贸n del c贸digo, de
forma que se espera una mejora con respecto a las entregas previas. Parte de
estas mejoras es gracias a que se tiene una mejor visi贸n de lo que se est谩
realizando.

Al igual que en los documentos ya presentados, este trabajo se encuentra
codificado en el lenguaje de programaci贸n Haskell. Se han hecho uso de
varias librer铆as para la implementaci贸n de las funciones, mismas que se
encuentran disponibles en Hackage. Esta elecci贸n tine un punto negativo
que ser谩 explicado m谩s adelante.

\section{Dise帽o del proyecto}
El programa se encuentra dividido en varias secciones internas:
\textit{Parsing}, \textit{Stage0}, \textit{Stage1}, \textit{Stage2}.
Cada una de est谩s secciones se encargan de llevar a cabo trabajos
espec铆ficos del procesado de los documentos. Adem谩s, se encuentra un
modulo \textit{main} que se encarga de la ejecuci贸n del programa.

\subsection{Parsing}
Para el \textit{Parsing} asumimos que el documento se encuentra
previamente almacenado en una estructura de datos de tipo \textit{Text}.
Ya que este proceso es de naturaleza impura, no se puede realizar
en esta secci贸n; \textit{main} se encargar谩 de ello.

Nuestro punto de entrada para esta secci贸n es la funci贸n 
\textit{parseDocument}. Esta funci贸n se encarga de extraer los datos
del archivo de texto y de preprocesarlos para que puedan ser utilizados
por el resto de las secciones.

El primer paso es hacer uso de la libraria \textit{Text.XML.Light} para
leer el archivo XML en una estructura manejable. El problema de hacer
esto directamente es que la librer铆a toma en cuenta los saltos de l铆nea
al momento de leer las etiquetas. Por lo tanto, se debe realizar un
filtro para eliminar todos aquellos objetos que no sean elementos con
informaci贸n relevante.

Una vez que tenemos est谩 divis铆on de datos, se procede a dividir los
datos en dos partes: la informaci贸n del documento y el texto de la
noticia. Esto se retorna en una lista de tuplas que puede ser accedida
por los siguientes pasos.

Lo siguiente a hacer es limpiar y preprocesar el texto de la noticia.
Para ello eliminamos todos los n煤meros de la noticia, los signos de
puntuaci贸n, las urls y se convierte el texto a min煤sculas. Posteriormente,
el texto es dividido en palabras y se filtra de forma que no se incluyan
\textit{stopwords}. Por 煤ltimo, se ensambla el texto en un solo
\textit{Text} y se da como entrada al algoritmo de \textit{stemming}.

El algoritmo de \textit{stemming} que estamos usando en este trabajo
es un proyecto antiguo, adem谩s de que se trata de una biblioteca que
hace \textit{ffi} a una biblioteca en C. Una de las complicaciones al
usar este algoritmo es que no se puede hacer uso de la librer铆a de
forma directa, sino que hay que modificar el c贸digo de la biblioteca
para que funcione en un compilador de Haskell m谩s moderno.

El algoritmo usado, a pesar de que es una biblioteca de C 芦antigua禄,
compila sin problemas en Darwin moderno.

\subsection{Stage0 y representaci贸n interna}
Lo m谩s interesante (e importante) de esta secci贸n es que se ha
dise帽ado un tipo producto para la representaci贸n de los datos que
nos permite tener un mejor control y acceso en las etapas siguientes.
Esta estructura est谩 definida como se muestra a continuaci贸n:

\lstinputlisting[firstline=13, lastline=19]{src/IndexRep.hs}
disponibles (\textit{\_\_docsN}) y la lista de las palabras que se
usan a lo largo de todos los documentos (\textit{\_\_words}). En
cuanto a \textit{\_\_docs}, se define como una representaci贸n de
indice invertido, pero manejando un \textit{Map} para tener orden
en el acceso. Est谩 ultima guarda las apariciones de cada palabra
en cada documento (si el documento la contiene).

Tener \textit{\_\_docs} y \textit{\_\_weights} puede parecer un
poco ineficiente. T茅cnicamente lo es, pero en este proyecto se
requiere para poder mostrar los datos calculados por el programa.
Adem谩s, en un principio requerimos de \textit{\_\_docs} para poder
tener suficiente informaci贸n para el calculo de \textit{\_\_weights}.
Por lo que pensamos que su uso (en este caso espec铆ficamente) es
justificado.

Adem谩s se definen una serie de \textit{lenses} y morfismos para
tener un acceso m谩s f谩cil a los datos de la estructura. Esto es una
mejora muy grande con respecto a las versiones anteriores que se
han realizado. Simplifica grandemente la claridad del c贸digo y
reduce la necesidad de declarar m谩s funciones.

En cuanto a la generaci贸n, en esta etapa s贸lo se inicializa la
estructura con el n煤mero de documentos y la lista de palabras en
orden. Adem谩s se establecen los mapas con listas vac铆as.

\subsection{Stage1}

En esta secci贸n no hay necesidad de una explicaci贸n muy extensa,
ya que se trata de una etapa que se encarga de llenar la estructura
con el n煤mero de apariciones de cada palabra en cada documento.
Simplemente suministramos el texto de cada documento y la
estructura de datos que se encuentra en la etapa anterior.
El resultado de esta etapa es una estructura de datos con la
informaci贸n actualizada.

\subsection{Stage2}

Aqu铆 nos ecargamos de calcular el peso de cada palabra en cada
documento. En este caso basta con proporcionar s贸lo la estructura
de datos de la etapa anterior, ya que contiene la informaci贸n
necesaria para calcular cada uno de los pesos.

\subsubsection{TF}
Definimos la funci贸n \textit{tf} que calcula la frecuencia de una
palabra en un documento de la siguiente forma:

\lstinputlisting[firstline=91, lastline=96]{src/IndexRep.hs}

\subsubsection{IDF}
Definimos la funci贸n \textit{idf} que calcula la frecuencia de una
palabra en todos los documentos de la siguiente forma:

\lstinputlisting[firstline=91, lastline=96]{src/IndexRep.hs}


\subsubsection{TF-IDF}
Finalmente, definimos la funci贸n \textit{tfidf} que calcula el
pesado del termino de la siguiente forma:

\lstinputlisting[firstline=98, lastline=100]{src/IndexRep.hs}

\subsubsection{Peso}
Simplemente basta con proveer de los indice necesarios (y de la
estructura de datos de la etapa anterior) para calcular el peso
de cada palabra. Esto se puede optener con facilidad usando los
lenses. Adem谩s de que la estructura de la funci贸n \textit{tfidf}
es m谩s similar a su definici贸n teor铆ca.

Al acabar el procesor para todos los indices, tenemos una estructura
actualizada con los valores para \textit{\_\_weights}
correspondientes a cada palabra.

\subsection{Similitud Coseno}
La similitud coseno es una de las m谩s importantes funciones que
se presentan en este proyecto, ya que es la que se encarga de
calcular la similitud entre dos documentos. Pero en este caso,
se calcul谩 todos los pares de documentos. Esta funci贸n se define
como sigue:

\lstinputlisting[firstline=74, lastline=78]{src/Glue.hs}
S贸lo hay que suministrarle los respectivos vectores de pesos
de cada documento a comparar.

Sin embargo, la funci贸n no se ejecuta directamente, sino que es
invocada por una funci贸n que se encarga de ejecutarla en cada
par de documentos. Esto se realiza facilmente con el uso de
una \textit{comprehension list} que da los resultados de todos
los c谩lculos.

\subsection{Ensamblaje (\textit{main})}
La longitud de los datos es demasiado grande para que se pueda
calcular directamente a mano. Por lo que se necesita una funci贸n
que se encarga de ejecutar todos los procesos necesarios para
calcular la similitud entre todos los documentos.

En este apartado nos encontramos con funciones que calculan
los datos de cada etapa y que se encargan de escribir los datos
en un archivo compilable por \LaTeX{}. Esto se realiza con
el uso del paquete \textit{HaTeX}, que nos permite escribir
archivos de texto en \LaTeX{} directamente desde el c贸digo
en Haskell.

Por lo que la impresi贸n de los datos en las siguientes secciones
es hecha program谩ticamente.

\section{Tabla del conteo de documentos y palabras}

{
\fontsize{2pt}{2pt}\selectfont

\input{table0.tex}

}

\section{Tabla del peso de documentos y palabras}

{
\fontsize{2pt}{2pt}\selectfont

\input{table1.tex}

}

\section{Tabla de la Similitud Coseno}


\resizebox{\textwidth}{!}{
{
\fontsize{2pt}{2pt}\selectfont

\input{table2.tex}

}
}

\section{Ejecuci贸n del programa}
El programa se ejecuta desde la l铆nea de comandos haciendo
uso de \textit{ghci}, o bien, compilando el programa con
\textit{ghc}. El resultado de la compilaci贸n nos da un
programa que es capaz de generar cada uno de los pasos y
de escribir los datos en un archivo \LaTeX{} compilable.

Para m谩s informaci贸n, consulte el archivo \textit{README.md}

\section{Conclusiones}
Algo importante que se debe tener en cuenta es que el proceso
de indexaci贸n es muy lento por la forma en la que se realiza
el c谩lculo de los textos. Por lo que es recomendable que
en una implementaci贸n m谩s eficiente, se haga uso de estructuras
que permitan el acceso de datos m谩s r谩pido.

Haskell es un lenguaje de programaci贸n inmutable, por lo que
no se puede modificar el contenido de una estructura de datos
sin crear una nueva. Este es un problema ya que nos obliga a
crear una nueva estructura para cada cambio que se realice.

Obviamente estos cambios afectan considerablemente al rendimiento
de la aplicaci贸n. Sin embargo, simepre hay alternativas que
podr铆an resolver este problema, como el uso de monadas, IO,
estructuras ligadas a tipos lineales, etc.

Pero todas estas alternativas son muy complicadas y no son
parte del alcance de este proyecto.

Otro punto a tomar en cuenta es que si bien el proceso de
similitud deber铆a ser muy r谩pido, esto no es lo que se ha
observado. En realidad el c谩lculo de la similitud es bastante
lento, siendo la parte m谩s lenta de toda la aplicaci贸n.

Esto se debe probablemente a que hay que generar una gran
cantidad de vectores. Un total de $101*101$ resultados
son generados, lo que es bastante grande y puede ser
complicado de calcular.

Estos tiempos son a煤n m谩s lentos cuando se ejecuta desde el
comando \textit{ghci}. Por lo que es recomendable que se
compile el programa con \textit{ghc} para que se ejecute
m谩s r谩pido la generaci贸n de los documentos.



\end{document}
